{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fewer-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_dist(dataset, feature_name, label, step):\n",
    "    # 左闭右开\n",
    "    if dataset is None:\n",
    "        return []\n",
    "    \n",
    "    min_val = min(dataset[feature_name])\n",
    "    max_val = max(dataset[feature_name]) \n",
    "    \n",
    "    #print('feature:{},min:{}, max:{}'.format(feature_name, min_val, max_val))\n",
    "    \n",
    "    #print('min:{}, max:{}'.format(min_val, max_val))\n",
    "    val = np.linspace(min_val, max_val, num=step+1)\n",
    "    val = val.tolist()\n",
    "    #print('val:{}'.format(val))\n",
    "    \n",
    "    dist=[]\n",
    "    deal_data = dataset.loc[dataset['label']==label]\n",
    "    if deal_data is None:\n",
    "        return []\n",
    "    sum_val = 0\n",
    "    for i in range (step):\n",
    "       # print('i:{},i+1:{}'.format(val[i], val[i+1]))\n",
    "        x = deal_data.loc[(deal_data[feature_name]>=val[i]) & (deal_data[feature_name]<val[i+1])]\n",
    "        sum_val = sum_val + len(x)\n",
    "        val_size = len(x)\n",
    "        dist.append(val_size)\n",
    "    \n",
    "    \n",
    "    for i, val in enumerate(dist):\n",
    "        if sum_val == 0:\n",
    "            break\n",
    "        dist[i] = dist[i]/sum_val\n",
    "        if dist[i] == 0:\n",
    "            dist[i] = 0.000001\n",
    "    return dist\n",
    "    \n",
    "\n",
    "def kl_divergence(feature_name, dataset):\n",
    "    '''\n",
    "    计算feature_name对应的kl散度\n",
    "    feature_name:特征名称\n",
    "    dataset 数据集合\n",
    "    面临一个问题，normal和disease两个数据长度不一致，需要对齐\n",
    "    就算长度对齐也不对，需要进行分桶，计算好分布\n",
    "    '''\n",
    "   # neg = array_to_dist(dataset, feature_name, 0, 20)\n",
    "    #pos = array_to_dist(dataset, feature_name, 1, 20)\n",
    "    \n",
    "    #print('neg:{},pos:{}'.format(neg, pos))\n",
    "    '''\n",
    "    这里使用numpy.histogram来代替之前自己写的函数，看看速度会提升不\n",
    "    '''\n",
    "    neg_feature_data = dataset[dataset['label'] == 0][feature_name] # 负样本数据\n",
    "    pos_feature_data = dataset[dataset['label'] == 1][feature_name] # 正样本数据\n",
    "    \n",
    "    min_value = pos_feature_data.min() if neg_feature_data.min() > pos_feature_data.min() else neg_feature_data.min()\n",
    "    max_value = neg_feature_data.max() if neg_feature_data.max() > pos_feature_data.max() else pos_feature_data.max()\n",
    "    neg_hist, neg_bin = np.histogram(neg_feature_data, bins = 10, range=(min_value, max_value), density = True)\n",
    "    pos_hist, pos_bin = np.histogram(pos_feature_data, bins = 10, range=(min_value, max_value), density = True)\n",
    "    pos_hist[pos_hist == 0] = 1e-7\n",
    "    neg_hist[neg_hist == 0] = 1e-7\n",
    "    return entropy(neg_hist, pos_hist, base = 2)\n",
    "\n",
    "# 获取所有df的\n",
    "def get_all_kl_divergence(df):\n",
    "    features =[]\n",
    "    feature_kl = []\n",
    "    size = len(df.columns)\n",
    "    i = 0\n",
    "    for feature in df.columns:\n",
    "        if feature == 'label':\n",
    "            continue\n",
    "        features.append(feature)\n",
    "        feature_kl.append(kl_divergence(feature, df))\n",
    "        i = i + 1\n",
    "        print('process:{}'.format(i))\n",
    "    kl_feature_dataset = pd.DataFrame()\n",
    "    kl_feature_dataset['feature_name'] = features\n",
    "    kl_feature_dataset['kl_divergence'] = feature_kl\n",
    "    final = kl_feature_dataset.sort_values('kl_divergence', ascending=False)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_hist(arr):\n",
    "    # 数组生成hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_train_v4.csv', index_col = 0)\n",
    "\n",
    "dd = get_all_kl_divergence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(feature_name, dataset):\n",
    "    '''\n",
    "     feature_name : 特征名称\n",
    "     dataset: 训练集合的dataFrame的全集\n",
    "    '''\n",
    "    if  dataset is None:\n",
    "        return\n",
    "    if feature_name is None:\n",
    "        return\n",
    "    # 获取最后正样本和负样本两种数据的差异\n",
    "    #ax.set_title('{}患病和正常组的数据分布图'.format(feature_name))\n",
    "    #f, ax= plt.subplots()\n",
    "    g = sns.displot(dataset, x=feature_name, kind='kde', hue='label', fill=True, rug=True)\n",
    "    #g.set_axis_labels('{}'.format(feature_name))\n",
    "    #g.set_titles('ontrol group and disease group PDF',)\n",
    "    #g.set_ylabels(\"control group and disease group PDF\")\n",
    "    g.ax.set_title(u'{} disease group and control group PDF'.format(feature_name))\n",
    "    #g.savefig(\"{}.png\".format(feature_name))\n",
    "    #sns.displot(disease_data,kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train('FXYD1', df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosed_features = dd[dd['kl_divergence'] > 3].reset_index(drop = True)\n",
    "choosed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(choosed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continent-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=20):\n",
    "  dataframe1 = dataframe.copy()\n",
    "  labels = dataframe1.pop('label')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe1), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe1))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "# 定义一个函数，输入数据，然后将列作为feature_column 然后构建lr模型来看哪个特征重要\n",
    "def train_model(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    if len(data.columns) == 0:\n",
    "        return None\n",
    "    feature_columns = []\n",
    "    for feature in data.columns:\n",
    "        if feature == 'label':\n",
    "            continue\n",
    "        feature_columns.append(tf.feature_column.numeric_column(feature))\n",
    "    \n",
    "    # 生成训练数据\n",
    "    batch_size=20\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "    test_ds = df_to_dataset(test, batch_size=batch_size)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.DenseFeatures(feature_columns),\n",
    "          tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC','accuracy', 'Recall', 'Precision'])\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    model.fit(train_ds,\n",
    "          validation_data = test_ds,\n",
    "          epochs = 100,\n",
    "          callbacks = [tensorboard_callback])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final = df[choosed_features['feature_name'] + ['label']]\n",
    "#df_final['label'] = df['label']\n",
    "#choosed_features['feature_name'].values.insert(['label'])\n",
    "#choosed_features['feature_name'].append('label')\n",
    "index_columns = np.append(choosed_features['feature_name'].values, ['label'])\n",
    "df_final = df[index_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2 = shuffle(df_final)\n",
    "train_model(df_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "primary-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_train_v4.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "df_t2 = shuffle(df)\n",
    "train_model(df_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-estimate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-guarantee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
